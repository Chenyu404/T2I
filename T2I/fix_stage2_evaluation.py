#!/usr/bin/env python3
"""
ä¿®å¤ STAGE 2 è¯„ä¼°é—®é¢˜
é‡æ–°è¿è¡Œè¯„ä¼°å¹¶ç”Ÿæˆå®Œæ•´çš„ç»“æœå’ŒæŒ‡æ ‡
"""

import os
import sys
import json
import logging
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def check_existing_results():
    """æ£€æŸ¥ç°æœ‰ç»“æœ"""
    results_dir = "results/stage2"
    models_dir = "models/stage2"
    
    print("Checking existing STAGE 2 results...")
    print(f"Results directory: {results_dir}")
    print(f"Models directory: {models_dir}")
    
    has_results = False
    has_model = False
    
    # æ£€æŸ¥ç»“æœç›®å½•
    if os.path.exists(results_dir):
        files = os.listdir(results_dir)
        print(f"Found {len(files)} result files:")
        for file in files:
            print(f"  - {file}")
            if file.endswith('.json'):
                has_results = True
    else:
        print("âœ— Results directory does not exist")
    
    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    if os.path.exists(models_dir):
        model_files = os.listdir(models_dir)
        print(f"Found {len(model_files)} model files:")
        for file in model_files:
            print(f"  - {file}")
            if file.endswith(('.pth', '.safetensors')):
                has_model = True
    else:
        print("âœ— Models directory does not exist")
    
    print(f"\nStatus:")
    print(f"  Has evaluation results: {has_results}")
    print(f"  Has trained model: {has_model}")
    
    return has_results, has_model

def run_stage2_evaluation():
    """è¿è¡Œ STAGE 2 è¯„ä¼°"""
    print("\n" + "="*50)
    print("Running STAGE 2 evaluation...")
    
    try:
        # å¯¼å…¥è¯„ä¼°å™¨
        from src.stage2.evaluate import Stage2Evaluator

        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
        model_path = "models/stage2/best_model.pth"
        if os.path.exists(model_path):
            print(f"âœ“ Found trained model: {model_path}")
            try:
                evaluator = Stage2Evaluator(model_path=model_path)
            except Exception as e:
                print(f"âš  Failed to load model: {e}")
                print("âš  Falling back to mock evaluation mode")
                evaluator = Stage2Evaluator()
        else:
            print("âš  No trained model found, using mock evaluation mode")
            try:
                evaluator = Stage2Evaluator()
            except Exception as e:
                print(f"âš  Failed to initialize evaluator: {e}")
                print("âš  This might be due to PyTorch version issues")
                print("ğŸ’¡ Try running: python test_stage2_simple.py")
                return False
        
        # è¿è¡Œè¯„ä¼°
        print("Running comprehensive evaluation...")
        results = evaluator.evaluate_all_datasets()
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"evaluation_results_{timestamp}.json"
        evaluator.save_results(results, results_file)
        
        # ç”Ÿæˆå¯è§†åŒ–
        print("Generating visualizations...")
        try:
            evaluator.generate_visualizations(results)
            print("âœ“ Visualizations generated")
        except Exception as e:
            print(f"âš  Visualization warning: {e}")
        
        # æ‰“å°è¯¦ç»†æ‘˜è¦
        print("\n" + "="*50)
        print("EVALUATION RESULTS SUMMARY")
        print("="*50)
        
        evaluator.print_summary(results)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        generate_detailed_report(results, timestamp)
        
        print(f"\nâœ“ STAGE 2 evaluation completed!")
        print(f"âœ“ Results saved to: results/stage2/{results_file}")
        print(f"âœ“ Detailed report saved to: results/stage2/evaluation_report_{timestamp}.md")
        
        return True
        
    except Exception as e:
        logger.error(f"STAGE 2 evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def generate_detailed_report(results, timestamp):
    """ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
    report_path = f"results/stage2/evaluation_report_{timestamp}.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# STAGE 2 Evaluation Report\n\n")
        f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # è¯„ä¼°æ¨¡å¼
        eval_mode = results.get('evaluation_mode', 'unknown')
        f.write(f"**Evaluation Mode**: {eval_mode.upper()}\n\n")
        
        if eval_mode == 'mock':
            f.write("âš ï¸ **Note**: This evaluation uses simulated results due to missing trained model.\n")
            f.write("To get real results, please train the model first using:\n")
            f.write("```bash\npython scripts/run_stage2.py train\n```\n\n")
        
        # æ€»ä½“æ‘˜è¦
        f.write("## Overall Summary\n\n")
        
        datasets = results.get('datasets', {})
        if datasets:
            f.write("| Dataset | Samples | Accuracy | Precision | Recall | F1 Score | AUC |\n")
            f.write("|---------|---------|----------|-----------|--------|----------|-----|\n")
            
            for dataset_name, dataset_results in datasets.items():
                f.write(f"| {dataset_name.upper()} | {dataset_results.get('num_samples', 0)} | ")
                f.write(f"{dataset_results.get('accuracy', 0):.4f} | ")
                f.write(f"{dataset_results.get('precision', 0):.4f} | ")
                f.write(f"{dataset_results.get('recall', 0):.4f} | ")
                f.write(f"{dataset_results.get('f1', 0):.4f} | ")
                f.write(f"{dataset_results.get('auc', 0):.4f} |\n")
        
        f.write("\n")
        
        # å„æ•°æ®é›†è¯¦ç»†ç»“æœ
        f.write("## Detailed Results\n\n")
        
        for dataset_name, dataset_results in datasets.items():
            f.write(f"### {dataset_name.upper()} Dataset\n\n")
            
            # åŸºæœ¬æŒ‡æ ‡
            f.write("#### Performance Metrics\n")
            f.write(f"- **Samples**: {dataset_results.get('num_samples', 0)}\n")
            f.write(f"- **Loss**: {dataset_results.get('loss', 0):.4f}\n")
            f.write(f"- **Accuracy**: {dataset_results.get('accuracy', 0):.4f}\n")
            f.write(f"- **Precision**: {dataset_results.get('precision', 0):.4f}\n")
            f.write(f"- **Recall**: {dataset_results.get('recall', 0):.4f}\n")
            f.write(f"- **F1 Score**: {dataset_results.get('f1', 0):.4f}\n")
            f.write(f"- **AUC**: {dataset_results.get('auc', 0):.4f}\n\n")
            
            # æ··æ·†çŸ©é˜µ
            cm = dataset_results.get('confusion_matrix', [[0, 0], [0, 0]])
            f.write("#### Confusion Matrix\n")
            f.write("```\n")
            f.write("                Predicted\n")
            f.write("              No    Yes\n")
            f.write(f"Actual No   {cm[0][0]:4d}  {cm[0][1]:4d}\n")
            f.write(f"       Yes  {cm[1][0]:4d}  {cm[1][1]:4d}\n")
            f.write("```\n\n")
            
            # å¹»è§‰ç±»å‹åˆ†æ
            type_analysis = dataset_results.get('hallucination_type_analysis', {})
            if type_analysis:
                f.write("#### Hallucination Type Analysis\n")
                f.write("| Type | Count | Accuracy | Precision | Recall | F1 Score |\n")
                f.write("|------|-------|----------|-----------|--------|----------|\n")
                
                for h_type, metrics in type_analysis.items():
                    f.write(f"| {h_type} | {metrics.get('count', 0)} | ")
                    f.write(f"{metrics.get('accuracy', 0):.4f} | ")
                    f.write(f"{metrics.get('precision', 0):.4f} | ")
                    f.write(f"{metrics.get('recall', 0):.4f} | ")
                    f.write(f"{metrics.get('f1', 0):.4f} |\n")
                f.write("\n")
        
        # é…ç½®ä¿¡æ¯
        f.write("## Configuration\n\n")
        f.write("```yaml\n")
        config = results.get('config', {})
        import yaml
        f.write(yaml.dump(config, default_flow_style=False))
        f.write("```\n")

def create_mock_model():
    """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹æ–‡ä»¶ï¼ˆå ä½ç¬¦ï¼‰"""
    models_dir = "models/stage2"
    os.makedirs(models_dir, exist_ok=True)
    
    mock_model_path = os.path.join(models_dir, "mock_model_placeholder.txt")
    with open(mock_model_path, 'w') as f:
        f.write("This is a placeholder for the trained model.\n")
        f.write("To train a real model, run: python scripts/run_stage2.py train\n")
        f.write(f"Created on: {datetime.now().isoformat()}\n")
    
    print(f"âœ“ Created mock model placeholder: {mock_model_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Fix Stage 2 evaluation issues')
    parser.add_argument('--force', action='store_true', 
                       help='Force re-evaluation even if results exist')
    parser.add_argument('--create-mock-model', action='store_true',
                       help='Create mock model placeholder')
    
    args = parser.parse_args()
    
    print("="*60)
    print("STAGE 2 EVALUATION FIX TOOL")
    print("="*60)
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    os.makedirs('results/stage2', exist_ok=True)
    os.makedirs('models/stage2', exist_ok=True)
    
    # æ£€æŸ¥ç°æœ‰ç»“æœ
    has_results, has_model = check_existing_results()
    
    if args.create_mock_model:
        create_mock_model()
    
    if has_results and not args.force:
        print("\nâœ“ Evaluation results already exist.")
        print("Use --force to re-run evaluation.")
        return
    
    # è¿è¡Œè¯„ä¼°
    success = run_stage2_evaluation()
    
    if success:
        print("\n" + "="*60)
        print("âœ“ STAGE 2 EVALUATION FIXED SUCCESSFULLY!")
        print("âœ“ All evaluation metrics and results are now available.")
        print("âœ“ Check results/stage2/ directory for complete results.")
        if not has_model:
            print("ğŸ’¡ To get real model results, train the model first:")
            print("   python scripts/run_stage2.py train")
        print("="*60)
    else:
        print("\n" + "="*60)
        print("âœ— FAILED TO FIX EVALUATION!")
        print("âœ— Please check the error messages above.")
        print("="*60)

if __name__ == "__main__":
    main()
