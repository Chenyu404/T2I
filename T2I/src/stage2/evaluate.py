"""
é˜¶æ®µäºŒè¯„ä¼°è„šæœ¬
è¯„ä¼°å¹»è§‰æ£€æµ‹æ¨¡å‹æ€§èƒ½
"""

import os
import json
import logging
import yaml
from typing import Dict, List, Tuple
import torch
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, 
    roc_auc_score, confusion_matrix, classification_report
)
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®ä¸ºéäº¤äº’å¼åç«¯ï¼Œé€‚ç”¨äºæœåŠ¡å™¨ç¯å¢ƒ
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from tqdm import tqdm

from .model import HallucinationDetectionModel
from .dataset import create_data_loaders

try:
    from safetensors.torch import load_file as safetensors_load
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

logger = logging.getLogger(__name__)


class Stage2Evaluator:
    """
    é˜¶æ®µäºŒè¯„ä¼°å™¨
    """
    
    def __init__(self, 
                 config_path: str = "config/stage2_config.yaml",
                 model_path: str = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.model_path = model_path
        
        # è®¾ç½®è®¾å¤‡
        self.device = self.config['device']
        if self.device == 'cuda' and not torch.cuda.is_available():
            self.device = 'cpu'
            logger.warning("CUDA not available, using CPU")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.results_dir = self.config['output']['results_dir']
        os.makedirs(self.results_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = HallucinationDetectionModel(self.config).to(self.device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.mock_mode = False
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            logger.warning("No model weights loaded - using mock evaluation mode")
            self.mock_mode = True

        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        try:
            self.train_loader, self.val_loader, self.test_loader = create_data_loaders(self.config)
        except Exception as e:
            logger.warning(f"Failed to load real data: {e}, using mock data")
            self.train_loader = self.val_loader = self.test_loader = None
            self.mock_mode = True
        
        logger.info("Stage2Evaluator initialized successfully")
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    
    def load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        logger.info(f"Loading model from {model_path}")

        # å°è¯•åŠ è½½safetensorsæ ¼å¼
        safetensors_path = model_path.replace('.pth', '.safetensors')
        if SAFETENSORS_AVAILABLE and os.path.exists(safetensors_path):
            try:
                state_dict = safetensors_load(safetensors_path)
                self.model.load_state_dict(state_dict)
                logger.info("Model loaded successfully (safetensors format)")
                return
            except Exception as e:
                logger.warning(f"Failed to load safetensors format: {e}, trying torch.load")

        # å›é€€åˆ°torch.load
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            logger.error("Please upgrade PyTorch to version 2.6+ or use safetensors format")
            raise
    
    def evaluate_dataset(self, data_loader, dataset_name: str = "test") -> Dict:
        """
        è¯„ä¼°æ•°æ®é›† - æ”¯æŒçœŸå®è¯„ä¼°å’Œæ¨¡æ‹Ÿè¯„ä¼°
        """
        if self.mock_mode or data_loader is None:
            return self._mock_evaluate_dataset(dataset_name)
        else:
            return self._real_evaluate_dataset(data_loader, dataset_name)

    def _mock_evaluate_dataset(self, dataset_name: str = "test") -> Dict:
        """
        æ¨¡æ‹Ÿè¯„ä¼°æ•°æ®é›† - ç”Ÿæˆåˆç†çš„è¯„ä¼°ç»“æœ
        """
        logger.info(f"Running mock evaluation for {dataset_name} dataset...")

        # æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆä¸åŒçš„æ€§èƒ½æŒ‡æ ‡
        if dataset_name == "train":
            # è®­ç»ƒé›†é€šå¸¸æœ‰æ›´å¥½çš„æ€§èƒ½
            base_accuracy = np.random.uniform(0.85, 0.95)
            base_f1 = np.random.uniform(0.82, 0.92)
            num_samples = np.random.randint(800, 1200)
        elif dataset_name == "val":
            # éªŒè¯é›†æ€§èƒ½ä¸­ç­‰
            base_accuracy = np.random.uniform(0.75, 0.85)
            base_f1 = np.random.uniform(0.72, 0.82)
            num_samples = np.random.randint(200, 400)
        else:  # test
            # æµ‹è¯•é›†æ€§èƒ½ç›¸å¯¹ä¿å®ˆ
            base_accuracy = np.random.uniform(0.70, 0.80)
            base_f1 = np.random.uniform(0.68, 0.78)
            num_samples = np.random.randint(200, 400)

        # ç”Ÿæˆç›¸å…³çš„æŒ‡æ ‡
        precision = base_f1 + np.random.uniform(-0.05, 0.05)
        recall = base_f1 + np.random.uniform(-0.05, 0.05)
        auc = base_accuracy + np.random.uniform(0.05, 0.15)

        # ç¡®ä¿æŒ‡æ ‡åœ¨åˆç†èŒƒå›´å†…
        accuracy = np.clip(base_accuracy, 0.0, 1.0)
        precision = np.clip(precision, 0.0, 1.0)
        recall = np.clip(recall, 0.0, 1.0)
        f1 = np.clip(base_f1, 0.0, 1.0)
        auc = np.clip(auc, 0.0, 1.0)

        # ç”Ÿæˆæ··æ·†çŸ©é˜µ
        true_positives = int(num_samples * 0.4 * recall)
        false_negatives = int(num_samples * 0.4) - true_positives
        false_positives = int(true_positives / precision) - true_positives if precision > 0 else 0
        true_negatives = num_samples - true_positives - false_negatives - false_positives

        confusion_matrix = [
            [true_negatives, false_positives],
            [false_negatives, true_positives]
        ]

        # ç”Ÿæˆæ¯ç±»æŒ‡æ ‡
        per_class_precision = [true_negatives/(true_negatives + false_negatives) if (true_negatives + false_negatives) > 0 else 0.0, precision]
        per_class_recall = [true_negatives/(true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0.0, recall]
        per_class_f1 = [2 * per_class_precision[0] * per_class_recall[0] / (per_class_precision[0] + per_class_recall[0]) if (per_class_precision[0] + per_class_recall[0]) > 0 else 0.0, f1]
        support = [true_negatives + false_positives, true_positives + false_negatives]

        # ç”Ÿæˆå¹»è§‰ç±»å‹åˆ†æ
        hallucination_types = self.config['evaluation']['hallucination_types']
        type_analysis = {}
        for h_type in hallucination_types:
            type_count = np.random.randint(10, 50)
            type_accuracy = np.random.uniform(0.6, 0.9)
            type_f1 = np.random.uniform(0.6, 0.85)
            type_precision = type_f1 + np.random.uniform(-0.05, 0.05)
            type_recall = type_f1 + np.random.uniform(-0.05, 0.05)

            type_analysis[h_type] = {
                'count': type_count,
                'accuracy': np.clip(type_accuracy, 0.0, 1.0),
                'precision': np.clip(type_precision, 0.0, 1.0),
                'recall': np.clip(type_recall, 0.0, 1.0),
                'f1': np.clip(type_f1, 0.0, 1.0)
            }

        # ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹æ•°æ®
        all_ids = [f"{dataset_name}_sample_{i}" for i in range(num_samples)]
        all_labels = np.random.choice([0, 1], size=num_samples, p=[0.6, 0.4]).tolist()

        # åŸºäºå‡†ç¡®ç‡ç”Ÿæˆé¢„æµ‹
        all_preds = []
        for label in all_labels:
            if np.random.random() < accuracy:
                all_preds.append(label)  # æ­£ç¡®é¢„æµ‹
            else:
                all_preds.append(1 - label)  # é”™è¯¯é¢„æµ‹

        # ç”Ÿæˆæ¦‚ç‡
        all_probs = []
        for i, (true_label, pred_label) in enumerate(zip(all_labels, all_preds)):
            if true_label == pred_label:
                # æ­£ç¡®é¢„æµ‹ï¼Œç»™é«˜ç½®ä¿¡åº¦
                prob = np.random.uniform(0.7, 0.95)
            else:
                # é”™è¯¯é¢„æµ‹ï¼Œç»™ä½ç½®ä¿¡åº¦
                prob = np.random.uniform(0.5, 0.7)

            if pred_label == 1:
                all_probs.append([1-prob, prob])
            else:
                all_probs.append([prob, 1-prob])

        # ç”Ÿæˆå¹»è§‰ç±»å‹æ ‡ç­¾
        all_hallucination_types = np.random.choice(
            hallucination_types + ['none'],
            size=num_samples
        ).tolist()

        results = {
            'dataset': dataset_name,
            'num_samples': num_samples,
            'loss': np.random.uniform(0.3, 0.8),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'per_class_metrics': {
                'precision': per_class_precision,
                'recall': per_class_recall,
                'f1': per_class_f1,
                'support': support
            },
            'confusion_matrix': confusion_matrix,
            'hallucination_type_analysis': type_analysis,
            'predictions': {
                'ids': all_ids,
                'labels': all_labels,
                'predictions': all_preds,
                'probabilities': all_probs,
                'hallucination_types': all_hallucination_types
            },
            'mock_evaluation': True  # æ ‡è®°è¿™æ˜¯æ¨¡æ‹Ÿè¯„ä¼°
        }

        logger.info(f"Mock evaluation completed for {dataset_name}: Accuracy={accuracy:.4f}, F1={f1:.4f}")
        return results

    def _real_evaluate_dataset(self, data_loader, dataset_name: str = "test") -> Dict:
        """
        çœŸå®è¯„ä¼°æ•°æ®é›† - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        self.model.eval()
        
        all_preds = []
        all_labels = []
        all_probs = []
        all_hallucination_types = []
        all_ids = []
        
        total_loss = 0.0
        criterion = torch.nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc=f"Evaluating {dataset_name}"):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                pixel_values = batch['pixel_values'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids, attention_mask, pixel_values)
                logits = outputs['logits']
                
                # è®¡ç®—æŸå¤±
                loss = criterion(logits, labels)
                total_loss += loss.item()
                
                # é¢„æµ‹
                probs = torch.softmax(logits, dim=-1)
                preds = torch.argmax(logits, dim=-1)
                
                # æ”¶é›†ç»“æœ
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                all_hallucination_types.extend(batch['hallucination_type'])
                all_ids.extend(batch['id'])
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(data_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_preds, average='weighted'
        )
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(
            all_labels, all_preds, average=None
        )
        
        # è®¡ç®—AUC
        try:
            all_probs = np.array(all_probs)
            auc = roc_auc_score(all_labels, all_probs[:, 1])
        except:
            auc = 0.0
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds)
        
        # æŒ‰å¹»è§‰ç±»å‹åˆ†æ
        type_analysis = self._analyze_by_hallucination_type(
            all_labels, all_preds, all_hallucination_types
        )
        
        results = {
            'dataset': dataset_name,
            'num_samples': len(all_labels),
            'loss': avg_loss,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'per_class_metrics': {
                'precision': precision_per_class.tolist(),
                'recall': recall_per_class.tolist(),
                'f1': f1_per_class.tolist(),
                'support': support.tolist()
            },
            'confusion_matrix': cm.tolist(),
            'hallucination_type_analysis': type_analysis,
            'predictions': {
                'ids': all_ids,
                'labels': all_labels,
                'predictions': all_preds,
                'probabilities': all_probs.tolist(),
                'hallucination_types': all_hallucination_types
            }
        }
        
        return results
    
    def _analyze_by_hallucination_type(self, 
                                     labels: List[int], 
                                     preds: List[int], 
                                     hallucination_types: List[str]) -> Dict:
        """
        æŒ‰å¹»è§‰ç±»å‹åˆ†ææ€§èƒ½
        
        Args:
            labels: çœŸå®æ ‡ç­¾
            preds: é¢„æµ‹æ ‡ç­¾
            hallucination_types: å¹»è§‰ç±»å‹
            
        Returns:
            æŒ‰ç±»å‹çš„åˆ†æç»“æœ
        """
        type_analysis = {}
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„å¹»è§‰ç±»å‹
        unique_types = list(set(hallucination_types))
        
        for h_type in unique_types:
            # æ‰¾åˆ°è¯¥ç±»å‹çš„æ‰€æœ‰æ ·æœ¬
            type_indices = [i for i, t in enumerate(hallucination_types) if t == h_type]
            
            if not type_indices:
                continue
            
            type_labels = [labels[i] for i in type_indices]
            type_preds = [preds[i] for i in type_indices]
            
            # è®¡ç®—è¯¥ç±»å‹çš„æŒ‡æ ‡
            type_accuracy = accuracy_score(type_labels, type_preds)
            type_precision, type_recall, type_f1, _ = precision_recall_fscore_support(
                type_labels, type_preds, average='weighted'
            )
            
            type_analysis[h_type] = {
                'count': len(type_indices),
                'accuracy': type_accuracy,
                'precision': type_precision,
                'recall': type_recall,
                'f1': type_f1
            }
        
        return type_analysis
    
    def evaluate_all_datasets(self) -> Dict:
        """è¯„ä¼°æ‰€æœ‰æ•°æ®é›† - æ”¯æŒçœŸå®å’Œæ¨¡æ‹Ÿæ¨¡å¼"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config,
            'datasets': {},
            'evaluation_mode': 'mock' if self.mock_mode else 'real'
        }

        if self.mock_mode:
            logger.info("Running mock evaluation for all datasets...")
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šè¯„ä¼°æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†
            results['datasets']['train'] = self.evaluate_dataset(None, "train")
            results['datasets']['val'] = self.evaluate_dataset(None, "val")
            results['datasets']['test'] = self.evaluate_dataset(None, "test")
        else:
            logger.info("Running real evaluation...")
            # çœŸå®æ¨¡å¼ï¼šåªè¯„ä¼°æœ‰æ•°æ®åŠ è½½å™¨çš„æ•°æ®é›†
            if self.train_loader:
                logger.info("Evaluating training set...")
                results['datasets']['train'] = self.evaluate_dataset(self.train_loader, "train")

            if self.val_loader:
                logger.info("Evaluating validation set...")
                results['datasets']['val'] = self.evaluate_dataset(self.val_loader, "val")

            if self.test_loader:
                logger.info("Evaluating test set...")
                results['datasets']['test'] = self.evaluate_dataset(self.test_loader, "test")

        return results
    
    def generate_visualizations(self, results: Dict):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        logger.info("Generating visualizations...")
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆå›¾è¡¨
        for dataset_name, dataset_results in results['datasets'].items():
            self._plot_confusion_matrix(dataset_results, dataset_name)
            self._plot_hallucination_type_analysis(dataset_results, dataset_name)
    
    def _plot_confusion_matrix(self, results: Dict, dataset_name: str):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = np.array(results['confusion_matrix'])
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['No Hallucination', 'Hallucination'],
                   yticklabels=['No Hallucination', 'Hallucination'])
        plt.title(f'Confusion Matrix - {dataset_name.title()} Set')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f'confusion_matrix_{dataset_name}.png'))
        plt.close()
    
    def _plot_hallucination_type_analysis(self, results: Dict, dataset_name: str):
        """ç»˜åˆ¶å¹»è§‰ç±»å‹åˆ†æå›¾è¡¨"""
        type_analysis = results['hallucination_type_analysis']
        
        if not type_analysis:
            return
        
        # å‡†å¤‡æ•°æ®
        types = list(type_analysis.keys())
        f1_scores = [type_analysis[t]['f1'] for t in types]
        counts = [type_analysis[t]['count'] for t in types]
        
        # åˆ›å»ºå­å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # F1åˆ†æ•°æŸ±çŠ¶å›¾
        ax1.bar(types, f1_scores)
        ax1.set_title(f'F1 Score by Hallucination Type - {dataset_name.title()} Set')
        ax1.set_xlabel('Hallucination Type')
        ax1.set_ylabel('F1 Score')
        ax1.tick_params(axis='x', rotation=45)
        
        # æ ·æœ¬æ•°é‡æŸ±çŠ¶å›¾
        ax2.bar(types, counts)
        ax2.set_title(f'Sample Count by Hallucination Type - {dataset_name.title()} Set')
        ax2.set_xlabel('Hallucination Type')
        ax2.set_ylabel('Count')
        ax2.tick_params(axis='x', rotation=45)
        
        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f'hallucination_type_analysis_{dataset_name}.png'))
        plt.close()
    
    def save_results(self, results: Dict, filename: str = None):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"evaluation_results_{timestamp}.json"
        
        output_path = os.path.join(self.results_dir, filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved to {output_path}")
    
    def print_summary(self, results: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print("STAGE 2 EVALUATION SUMMARY")
        print("="*60)

        # æ˜¾ç¤ºè¯„ä¼°æ¨¡å¼
        eval_mode = results.get('evaluation_mode', 'unknown')
        if eval_mode == 'mock':
            print("ğŸ“‹ EVALUATION MODE: MOCK (Simulated Results)")
            print("   Note: Using simulated data due to missing trained model")
        else:
            print("ğŸ”¬ EVALUATION MODE: REAL (Actual Model Results)")
        print()
        
        for dataset_name, dataset_results in results['datasets'].items():
            print(f"\n{dataset_name.upper()} SET:")
            print(f"  Samples: {dataset_results['num_samples']}")
            print(f"  Accuracy: {dataset_results['accuracy']:.4f}")
            print(f"  Precision: {dataset_results['precision']:.4f}")
            print(f"  Recall: {dataset_results['recall']:.4f}")
            print(f"  F1 Score: {dataset_results['f1']:.4f}")
            print(f"  AUC: {dataset_results['auc']:.4f}")
            
            # å¹»è§‰ç±»å‹åˆ†æ
            type_analysis = dataset_results['hallucination_type_analysis']
            if type_analysis:
                print(f"  \nHallucination Type Analysis:")
                for h_type, metrics in type_analysis.items():
                    print(f"    {h_type}: F1={metrics['f1']:.4f}, Count={metrics['count']}")
        
        print("\n" + "="*60)
