"""
é˜¶æ®µäºŒè¿è¡Œè„šæœ¬
æ‰§è¡Œå¹»è§‰æ£€æµ‹æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
"""

import os
import sys
import logging
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from src.stage2.train import Stage2Trainer
from src.stage2.evaluate import Stage2Evaluator


def setup_logging(log_level: str = "INFO"):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f'logs/stage2_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        ]
    )


def train_model(args):
    """è®­ç»ƒæ¨¡å‹"""
    logger = logging.getLogger(__name__)
    logger.info("Starting Stage 2 training...")
    
    try:
        trainer = Stage2Trainer(args.config)
        trainer.train()
        
        logger.info("Training completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        return False


def evaluate_model(args):
    """è¯„ä¼°æ¨¡å‹"""
    logger = logging.getLogger(__name__)
    logger.info("Starting Stage 2 evaluation...")
    
    try:
        evaluator = Stage2Evaluator(args.config, args.model_path)
        
        # è¿è¡Œè¯„ä¼°
        results = evaluator.evaluate_all_datasets()
        
        # ç”Ÿæˆå¯è§†åŒ–
        if args.generate_plots:
            evaluator.generate_visualizations(results)
        
        # ä¿å­˜ç»“æœ
        evaluator.save_results(results)
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_summary(results)
        
        logger.info("Evaluation completed successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Evaluation failed: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Run Stage 2: Hallucination Detection Model Training')
    
    parser.add_argument('mode', choices=['train', 'eval', 'both'],
                       help='Mode to run: train, eval, or both')
    parser.add_argument('--config', type=str, default='config/stage2_config.yaml',
                       help='Configuration file path')
    parser.add_argument('--model-path', type=str, default=None,
                       help='Model path for evaluation (default: best model from config)')
    parser.add_argument('--generate-plots', action='store_true',
                       help='Generate visualization plots')
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='Logging level')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs('logs', exist_ok=True)
    setup_logging(args.log_level)
    
    logger = logging.getLogger(__name__)
    logger.info("Starting Stage 2")
    logger.info(f"Arguments: {args}")
    
    success = True
    
    # è®­ç»ƒæ¨¡å¼
    if args.mode in ['train', 'both']:
        success &= train_model(args)
        
        # å¦‚æœè®­ç»ƒæˆåŠŸä¸”æ¨¡å¼æ˜¯bothï¼Œè®¾ç½®æ¨¡å‹è·¯å¾„ä¸ºæœ€ä½³æ¨¡å‹
        if success and args.mode == 'both' and args.model_path is None:
            # ä»é…ç½®ä¸­è·å–æ¨¡å‹ä¿å­˜ç›®å½•
            import yaml
            with open(args.config, 'r') as f:
                config = yaml.safe_load(f)
            model_dir = config['output']['model_save_dir']
            args.model_path = os.path.join(model_dir, 'best_model.pth')
    
    # è¯„ä¼°æ¨¡å¼
    if args.mode in ['eval', 'both']:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
        if args.model_path is None:
            import yaml
            with open(args.config, 'r') as f:
                config = yaml.safe_load(f)
            model_dir = config['output']['model_save_dir']
            default_model_path = os.path.join(model_dir, 'best_model.pth')

            if os.path.exists(default_model_path):
                args.model_path = default_model_path
                logger.info(f"Using default model path: {args.model_path}")
            else:
                logger.warning("No model path specified and no default model found")
                logger.info("Will use mock evaluation mode")

        success &= evaluate_model(args)
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "="*50)
    if success:
        print("STAGE 2 COMPLETED SUCCESSFULLY!")
        
        if args.mode in ['train', 'both']:
            print("âœ“ Model training completed")
        
        if args.mode in ['eval', 'both']:
            print("âœ“ Model evaluation completed")
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ¨¡æ‹Ÿæ¨¡å¼
            if not args.model_path or not os.path.exists(args.model_path):
                print("ğŸ“‹ Note: Used mock evaluation mode (no trained model found)")
                print("ğŸ’¡ To get real results, train the model first: python scripts/run_stage2.py train")

        print(f"Results saved to: results/stage2/")
        
    else:
        print("STAGE 2 FAILED!")
        print("Check logs for details.")
        sys.exit(1)
    
    print("="*50)


if __name__ == "__main__":
    main()
